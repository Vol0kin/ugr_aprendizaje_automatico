\documentclass[11pt,a4paper]{article}
\usepackage[spanish,es-nodecimaldot]{babel}	% Utilizar español
\usepackage[utf8]{inputenc}					% Caracteres UTF-8
\usepackage{graphicx}						% Imagenes
\usepackage[hidelinks]{hyperref}			% Poner enlaces sin marcarlos en rojo
\usepackage{fancyhdr}						% Modificar encabezados y pies de pagina
\usepackage{float}							% Insertar figuras
\usepackage[textwidth=390pt]{geometry}		% Anchura de la pagina
\usepackage[nottoc]{tocbibind}				% Referencias (no incluir num pagina indice en Indice)
\usepackage{enumitem}						% Permitir enumerate con distintos simbolos
\usepackage[T1]{fontenc}					% Usar textsc en sections
\usepackage{amsmath}						% Símbolos matemáticos
\usepackage{algpseudocode}
\usepackage{algorithm}

% no accents in math operators
\unaccentedoperators

% Comando para poner el nombre de la asignatura
\newcommand{\asignatura}{Aprendizaje Automático}
\newcommand{\autor}{Vladislav Nikolov Vasilev}

% Comandos utilies
\newcommand{\answer}{\noindent\textbf{Solución}}
\newcommand{\ein}{E$_{in}$}
\newcommand{\eout}{E$_{out}$}
\newcommand{\addtoc}[1]{\addcontentsline{toc}{section}{#1}}

% Configuracion de encabezados y pies de pagina
\pagestyle{fancy}
\lhead{\autor{}}
\rhead{\asignatura{}}
\lfoot{Grado en Ingeniería Informática}
\cfoot{}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}		% Linea cabeza de pagina
\renewcommand{\footrulewidth}{0.4pt}		% Linea pie de pagina

\begin{document}
\pagenumbering{gobble}

% Pagina de titulo
\begin{titlepage}

\begin{minipage}{\textwidth}

\centering

\includegraphics[scale=0.5]{img/ugr.png}\\

\textsc{\Large \asignatura{}\\[0.2cm]}
\textsc{GRADO EN INGENIERÍA INFORMÁTICA}\\[1cm]

\noindent\rule[-1ex]{\textwidth}{1pt}\\[1.5ex]
\textsc{{\Huge TRABAJO 3\\[0.5ex]}}
\textsc{{\Large Cuestiones de Teoría\\}}
\noindent\rule[-1ex]{\textwidth}{2pt}\\[3.5ex]

\end{minipage}

\vspace{0.5cm}

\begin{minipage}{\textwidth}

\centering

\textbf{Autor}\\ {\autor{}}\\[2.5ex]
\textbf{Rama}\\ {Computación y Sistemas Inteligentes}\\[2.5ex]
\vspace{0.3cm}

\includegraphics[scale=0.3]{img/etsiit.jpeg}

\vspace{0.7cm}
\textsc{Escuela Técnica Superior de Ingenierías Informática y de Telecomunicación}\\
\vspace{1cm}
\textsc{Curso 2018-2019}
\end{minipage}
\end{titlepage}

\pagenumbering{arabic}
\tableofcontents
\thispagestyle{empty}				% No usar estilo en la pagina de indice

\newpage

\setlength{\parskip}{1em}

\section*{Ejercicio 1}
\addtoc{Ejercicio 1}

\noindent ¿Podría considerarse Bagging como una técnica para estimar el error de predicción de un
modelo de aprendizaje? Diga si o no con argumentos. En caso afirmativo compárela con
validación cruzada.

\answer

\section*{Ejercicio 2}
\addtoc{Ejercicio 2}

\noindent Considere que dispone de un conjunto de datos linealmente separable. Recuerde que una
vez establecido un orden sobre los datos, el algoritmo perceptron encuentra un hiperplano
separador interando sobre los datos y adaptando los pesos de acuerdo al algoritmo

\begin{algorithm}[H]
\caption{Perceptron}
\begin{algorithmic}[1]
\State \textbf{Entradas}: $(\mathbf{x}_i, y_i) = 1, \dots, n \; , \; w=0, \; k = 0$
\Repeat
	\State $k \gets (k + 1) \; \mod \; n$
	\If{$\text{sign}(y_i) \neq \text{sign}(\mathbf{w}^T\mathbf{x}_i)$}
		\State $\mathbf{w} \gets \mathbf{w} + y_i\mathbf{x}_i$
	\EndIf
\Until{todos los puntos bien clasificados}
\end{algorithmic}
\end{algorithm}

\noindent Modificar este pseudo-código para adaptarlo a un algoritmo simple de SVM, considerando
que en cada iteración adaptamos los pesos de acuerdo al caso peor clasificado de toda la
muestra. Justificar adecuadamente/matematicamente el resultado, mostrando que al final
del entrenamiento solo estaremos adaptando los vectores soporte.

\answer

\section*{Ejercicio 3}
\addtoc{Ejercicio 3}

\noindent Considerar un modelo SVM y los siguientes datos de entrenamiento: Clase-1:$\lbrace (1,1),$
$(2,2),(2,0) \rbrace$, Clase-2:$\lbrace (0,0),(1,0),(0,1) \rbrace$

\begin{enumerate}[label=\textit{\alph*})]
	\item Dibujar los puntos y construir por inspección el vector de pesos para el hiperplano óptimo y el margen óptimo.
\end{enumerate}

\answer

Primero vamos a dibujar los puntos para ver como se ditribuyen en el espacio:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/points.png}
\caption{Dibujo con los puntos de las dos clases en el espacio.}
\end{figure}

Se puede ver claramente que los puntos de los dos clases son linealmente separables, ya que perfectamente se pueden separar mediante
un hiperplano que pase en medio de ellas.

Para intentar obtener un hiperplano óptimo, vamos a suponer que éste tiene que pasar entre los puntos de las dos clases que estén más
cerca entre sí (es decir, que tiene que pasar entre los vectores soporte). Estos puntos son, para la Clase-1, el $(1, 1)$ y el
$(2, 0)$, y para la Clase-2 son el $(0,1)$ y el $(1, 0)$. Por tanto, sabiendo que el hiperplano óptimo tiene que pasar entre estos
puntos, dejando la mayor cantidad de margen a cada lado, podemos suponer que pasará justo en el punto medio para cada par de puntos
que están a la misma altura y son de clases diferentes. Es decir, que para los puntos $(0, 1)$ y $(1, 1)$ (los cuáles son de
diferente clase), sabemos que seguramente ese hiperplano pasará por el $(0.5, 1)$. Para los dos puntos de abajo, sabiendo que tiene
que pasar entre los puntos $(0, 0)$ y $(1, 0)$, seguramente pasará por el punto $(1.5, 0)$, el cuál está justo en medio de los
dos anteriores. Esto se puede ver mejor en la siguiente imagen:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/middle.png}
\caption{Dibujo con los puntos medios entre los vectores soporte de las dos clases, representados en negro.}
\end{figure}

Ahora lo único que nos queda es obtener el hiperplano que separa las dos clases. Como ya sabemos los puntos por los que puede pasar,
lo único que tenemos que obtener es la recta que pasa por esos dos puntos. Para eso podemos partir de la ecuación de la recta, la cuál
viene dada por la forma:

\begin{equation}
\label{line}
y = ax + b
\end{equation}

\noindent donde $a$ es la pendiente de la recta y $b$ el término independiente. Sustituyendo los valores de los puntos por $x$ e $y$
en la expresión dada por \eqref{line}, obtenemos el siguiente sistema de ecuaciones:

\begin{equation}
\left.
\begin{aligned}
	1 &= 0.5a + b \\
	0 &= 1.5a + b
\end{aligned}
\right\rbrace
\end{equation}

Ahora resolvemos el sistema de ecuaciones para obtener la solución:

\begin{equation}
\left.
\begin{aligned}
	1 - 0.5a &= b \\
	-1.5a&= b
\end{aligned}
\right\rbrace
\end{equation}

\begin{equation}
\begin{aligned}
1 - 0.5a &= -1.5a \\
1 &= -a \\
\end{aligned}
\end{equation}

De aquí, obtenemos que:

\begin{equation}
\label{eq:answer}
\left.
\begin{aligned}
a = -1 \\
b = 1.5
\end{aligned}
\right\rbrace
\end{equation}

Y finalmente, con los resultados obtenidos en \eqref{eq:answer}, sustityendo en la expresión dada en \eqref{line}, obtenemos que la
ecuación de la recta es la siguiente:

\begin{equation}
y = -x + 1.5
\end{equation}

Esta recta es, en un principio, el hiperplano óptimo que separa las dos clases. Para obtener los márgenes, lo único que tenemos que
hacer es obtener rectas paralelas a las del hiperplano que pasen por los vectores soporte. Para ello, lo único que tenemos que
modificar es el valor de $b$ que hemos obtenido con tal de obtener cada margen (el coeficiente libre indica el desplazamiento
en el eje $X$ que hace la recta para cortar con este eje en $x = 0$). En los dos casos es muy fácil obtener estos valores de $b$,
ya que algunos de los vectores soporte están sobre el eje $X$. Por tanto, tenemos que para la Clase-1, la recta que representa el
margen es la siguiente:

\begin{equation}
y = -x + 2
\end{equation}

Para la Clase-2, la recta es la siguiente:

\begin{equation}
y = -x + 1
\end{equation}

Por tanto, veamos como quedaría gráficamente el resultado de pintar el hiperplano óptimo y los márgenes:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/hyperplane.png}
\caption{Dibujo del hiperplano óptimo con las dos clases y los márgenes para cada clase.}
\end{figure}

\begin{enumerate}[resume,label=\textit{\alph*})]
	\item ¿Cuáles son los vectores soporte?
\end{enumerate}

\answer

Los vectores soporte son los siguientes:

\begin{itemize}[label=\textbullet]
	\item Para la \textbf{Clase-1}, los vectores soporte son $(1, 1)$ y $(2, 0)$.
	\item Para la \textbf{Clase-2}, los vectores soporte son $(0, 1)$ y $(1, 0)$.
\end{itemize}

\begin{enumerate}[resume,label=\textit{\alph*})]
	\item Construir la solución en el espacio dual. Comparar la solución con la del apartado (a)
\end{enumerate}

\answer



\section*{Ejercicio 4}
\addtoc{Ejercicio 4}

\noindent ¿Cúal es el criterio de optimalidad en la construcción de un árbol? Analice un clasificador
en árbol en términos de sesgo y varianza. ¿Que estrategia de mejora propondría?

\answer

\section*{Ejercicio 5}
\addtoc{Ejercicio 5}

\noindent ¿Cómo influye la dimensión del vector de entrada en los modelos: SVM, RF, Boosting and
NN?

\answer

\section*{Ejercicio 6}
\addtoc{Ejercicio 6}

\noindent El método de Boosting representa una forma alternativa en la búsqueda del mejor clasificador
respecto del enfoque tradicional implementado por los algoritmos PLA, SVM, NN, etc. a)
Identifique de forma clara y concisa las novedades del enfoque; b) Diga las razones profundas
por las que la técnica funciona produciendo buenos ajustes (no ponga el algoritmo); c)
Identifique sus principales debilidades; d) ¿Cuál es su capacidad de generalización comparado
con SVM?

\answer

\section*{Ejercicio 7}
\addtoc{Ejercicio 7}

\noindent Discuta pros y contras de los clasificadores SVM y Random Forest (RF). Considera que
SVM por su construcción a través de un problema de optimización debería ser un mejor
clasificador que RF. Justificar las respuestas.

\answer

\section*{Ejercicio 8}
\addtoc{Ejercicio 8}

\noindent ¿Cuál es a su criterio lo que permite a clasificadores como Random Forest basados en
un conjunto de clasificadores simples aprender de forma más eficiente? ¿Cuales son las
mejoras que introduce frente a los clasificadores simples? ¿Es Random Forest óptimo en
algún sentido? Justifique con precisión las contestaciones.

\answer

\section*{Ejercicio 9}
\addtoc{Ejercicio 9}

\noindent En un experimento para determinar la distribución del tamaño de los peces en un lago, se
decide echar una red para capturar una muestra representativa. Así se hace y se obtiene
una muestra suficientemente grande de la que se pueden obtener conclusiones estadísticas
sobre los peces del lago. Se obtiene la distribución de peces por tamaño y se entregan las
conclusiones. Discuta si las conclusiones obtenidas servirán para el objetivo que se persigue
e identifique si hay algo que lo impida.

\answer

\section*{Ejercicio 10}
\addtoc{Ejercicio 10}

\noindent Identifique que pasos daría y en que orden para conseguir con el menor esfuerzo posible un
buen modelo de red neuronal a partir una muestra de datos. Justifique los pasos propuestos,
el orden de los mismos y argumente que son adecuados para conseguir un buen óptimo.
Considere que tiene suficientes datos tanto para el ajuste como para el test.

\answer

\newpage

\begin{thebibliography}{5}

\bibitem{nombre-referencia}
Texto referencia
\\\url{https://url.referencia.com}

\end{thebibliography}

\end{document}

